{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Ensmble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-off\n",
    "\n",
    "$\\newcommand{\\coloneqq}{\\mathrel{\\vcenter{:}}=}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\y}{\\mathbf{y}}$\n",
    "\n",
    "Let us compute the bias-variance trade-off graph for a problem of polynomial fitting. Recall, that the error decomposition for the MSE loss function is: $$ MSE_{\\y}\\left(\\widehat{\\y}\\right)=\\E\\left[\\left(\\widehat{\\y}-\\y^*\\right)^2\\right] = Var\\left(\\widehat{\\y}\\right) + Bias^2\\left(\\widehat{\\y}\\right) $$\n",
    "\n",
    "Where the bias and variances of estimators are defined as: $$ Bias\\left(\\widehat{\\y}\\right) \\coloneqq \\E\\left[\\widehat{\\y}\\right] - \\y, \\quad Var\\left(\\widehat{\\y}\\right)\\coloneqq \\E\\left[\\left(\\widehat{\\y}-\\E\\left[\\widehat{\\y}\\right]\\right)^2\\right]$$\n",
    "\n",
    "As the $\\E\\left[\\widehat{\\y}\\right]$ is over the selection of the training sets, we will first defined the \"ground truth\" model and retrieve a set $\\mathbf{X},\\y$ from it. Then, we will repeatedly sample Gaussian noise $\\varepsilon$ and fit a polynomial model over $\\mathbf{X},\\y+\\varepsilon$. In the code below `y_` denotes the true $\\y$ values and `y` the responses after adding the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../figures/bias_variance_poly.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-28333bc5f50e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     49\u001B[0m                          \u001B[0mxaxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtitle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mr\"$\\text{Degree of Fitted Polymonial}$\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m                          width=800, height=500))\n\u001B[1;32m---> 51\u001B[1;33m \u001B[0mfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_image\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"../figures/bias_variance_poly.png\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m \u001B[0mfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\plotly\\basedatatypes.py\u001B[0m in \u001B[0;36mwrite_image\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3804\u001B[0m         \u001B[1;32mimport\u001B[0m \u001B[0mplotly\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpio\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3805\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3806\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mpio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_image\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3807\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3808\u001B[0m     \u001B[1;31m# Static helpers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\plotly\\io\\_kaleido.py\u001B[0m in \u001B[0;36mwrite_image\u001B[1;34m(fig, file, format, scale, width, height, validate, engine)\u001B[0m\n\u001B[0;32m    256\u001B[0m     \u001B[1;31m# ---------\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfile_is_str\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 258\u001B[1;33m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"wb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    259\u001B[0m             \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    260\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../figures/bias_variance_poly.png'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data according to a polynomial model of degree 4\n",
    "model = lambda x: x**4 - 2*x**3 - .5*x**2 + 1\n",
    "X = np.linspace(-1.6, 2, 60)\n",
    "y = model(X).astype(np.float64)\n",
    "X_train, X_test, y_train_, y_test_ = train_test_split(X, y, test_size=.5, random_state=13)\n",
    "\n",
    "\n",
    "# The following functions recieve two matrices of the true values and the predictions\n",
    "# where rows represent different runs and columns the different responses in the run\n",
    "def variance(y_pred):\n",
    "    return np.mean(np.var(y_pred - np.mean(y_pred, axis=0), axis=0, ddof=1))\n",
    "\n",
    "def bias(y_pred, y_true):\n",
    "    mean_y = y_pred.mean(axis=0)\n",
    "    return np.mean((mean_y - y_true)**2)\n",
    "\n",
    "def error(y_pred, y):\n",
    "    return np.mean((y_pred - y)**2)\n",
    "\n",
    "\n",
    "\n",
    "ks, repetitions = list(range(11)), 100\n",
    "biases, variances, errors = np.zeros(len(ks)), np.zeros(len(ks)), np.zeros(len(ks))\n",
    "for i, k in enumerate(ks):\n",
    "    # Add noise to train and test samples\n",
    "    y_train = y_train_[np.newaxis, :] + np.random.normal(0, 3, size=(repetitions, len(y_train_)))\n",
    "    y_test  = y_test_ + np.random.normal(size=len(y_test_))\n",
    "    \n",
    "    # Fit model multiple times (each time over a slightly different training sample) and predict over test set\n",
    "    y_preds = np.array([make_pipeline(PolynomialFeatures(k), LinearRegression())\\\n",
    "                            .fit(X_train.reshape(-1,1), y_train[j,:])\\\n",
    "                            .predict(X_test.reshape(-1,1))\n",
    "                        for j in range(repetitions)])\n",
    "    \n",
    "    biases[i], variances[i], errors[i] = bias(y_preds, y_test_), variance(y_preds), error(y_preds, y_test_)\n",
    "\n",
    "\n",
    "fig = go.Figure([\n",
    "            go.Scatter(x=ks, y=biases, name=r\"$Bias^2$\"),\n",
    "            go.Scatter(x=ks, y=variances, name=r\"$Variance$\"),\n",
    "            go.Scatter(x=ks, y=biases+variances, name=r\"$Bias^2+Variance$\"),\n",
    "            go.Scatter(x=ks, y=errors, name=r\"$Generalization\\,\\,Error$\")], \n",
    "        layout=go.Layout(title=r\"$\\text{Generalization Error Decomposition - Bias-Variance of Polynomial Fitting}$\",\n",
    "                         xaxis=dict(title=r\"$\\text{Degree of Fitted Polymonial}$\"),\n",
    "                         width=800, height=500))\n",
    "fig.write_image(f\"../figures/bias_variance_poly.png\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committee Decisions\n",
    "\n",
    "Let $X_1,\\ldots,X_T\\overset{iid}{\\sim}Ber\\left(p\\right)$ taking values in $\\left\\{\\pm1\\right\\}$, with the probability of each being correct being $p>0.5$. We can bound the probability of the committee being correct by: $$\\mathbb{P}\\left(\\sum X_i > 0\\right) \\geq 1-\\exp\\left(-\\frac{T}{2p}\\left(p-\\frac{1}{2}\\right)^2\\right)$$\n",
    "\n",
    "Let us show this bounding below empirically by sampling increasing amount of such Bernoulli random variables, and to do so for different values of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bound = np.vectorize(lambda p, T: 1-np.exp(-(T/(2*p))*(p-.5)**2))\n",
    "\n",
    "ps  = np.concatenate([[.5001], np.linspace(.55, 1, 14)])\n",
    "Ts = [1,5,10,15,20,25,50,75,100,125,150,175,200,250,300,400,500,600]\n",
    "\n",
    "frames = []\n",
    "for p in ps:\n",
    "    theoretical = bound(p,Ts)\n",
    "    empirical = np.array([[np.sum(np.random.choice([1, -1], T, p=[p, 1-p])) > 0 for _ in range(100)] for T in Ts])\n",
    "    \n",
    "    frames.append(go.Frame(data=[go.Scatter(x=Ts, y=theoretical, mode=\"markers+lines\", name=\"Theoretical Bound\",\n",
    "                                            line=dict(color=\"grey\", dash='dash')),\n",
    "                                 go.Scatter(x=Ts, y=empirical.mean(axis=1), \n",
    "                                            error_y = dict(type=\"data\", array=empirical.var(axis=1)),\n",
    "                                            mode=\"markers+lines\", marker_color=\"black\", name=\"Empirical Probability\")],\n",
    "                           layout=go.Layout(\n",
    "                               title_text=r\"$\\text{{Committee Correctness Probability As Function of }}\\\n",
    "                               T\\text{{: }}p={0}$\".format(round(p,3)),\n",
    "                               xaxis=dict(title=r\"$T \\text{ - Committee Size}$\"),\n",
    "                               yaxis=dict(title=r\"$\\text{Probability of Being Correct}$\", range=[0.0001,1.01]))))\n",
    "\n",
    "\n",
    "fig = go.Figure(data=frames[0][\"data\"],\n",
    "        frames=frames[1:], \n",
    "        layout=go.Layout(\n",
    "            title=frames[0][\"layout\"][\"title\"],\n",
    "            xaxis=frames[0][\"layout\"][\"xaxis\"],\n",
    "            yaxis=frames[0][\"layout\"][\"yaxis\"],\n",
    "            updatemenus=[dict(type=\"buttons\", buttons=[AnimationButtons.play(frame_duration=1000), \n",
    "                                                       AnimationButtons.pause()])] ))\n",
    "\n",
    "animation_to_gif(fig, \"../figures/committee_decision_correctness.gif\", 700, width=600, height=450)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, of uncorrelated committee members, we have shown the variance in the committee decision is: $$ Var\\left(\\sum X_i\\right) = \\frac{4}{T}p\\left(1-p\\right)$$\n",
    "Let us simulate such a scenario and see what is the empirical variance we achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps  = np.concatenate([[.5001], np.linspace(.55, 1, 10)])\n",
    "Ts = [1,5,10,15,20,25,50,75,100,125,150,175,200,250,300,400,500,600]\n",
    "\n",
    "    results = np.array([np.var(np.random.binomial(Ts, p, (10000, len(Ts))) >= (np.array(Ts)/2), axis=0, ddof=1) for p in ps])\n",
    "\n",
    "df = pd.DataFrame(results, columns=Ts, index=ps)\n",
    "fig = go.Figure(go.Heatmap(x=df.columns.tolist(), y=df.index.tolist(), z=df.values.tolist(), colorscale=\"amp\"),\n",
    "          layout=go.Layout(title=r\"$\\text{Variance of Committee Decision - Independent Members}$\", \n",
    "                           xaxis=dict(title=r\"$T\\text{ - Committee Size}$\", type=\"category\"),\n",
    "                           yaxis=dict(title=r\"$p\\text{ - Member Correctness Probability}$\"),\n",
    "                           width=800, height=500))\n",
    "\n",
    "fig.write_image(\"../figures/uncorrelated_committee_decision.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a set of correlated random variables, with correlation coefficient of $\\rho$ and variance of $\\sigma^2$, the variane of the committee's decision is: $$ Var\\left(\\sum X_i\\right) = \\rho \\sigma^2 + \\frac{1}{T}\\left(1-\\rho\\right)\\sigma^2 $$\n",
    "Let us set $\\sigma^2$ and investigate the relation between $\\rho$ and $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma = round((lambda p: p*(1-p))(.6), 3)\n",
    "repeats = 10000\n",
    "rho = np.linspace(0,1, 10)\n",
    "Ts = np.array([1,5,10,15,20,25,50,75,100,125,150,175,200,250,300,400,500,600])\n",
    "\n",
    "variances = np.zeros((len(rho), len(Ts)))\n",
    "for i, r in enumerate(rho):\n",
    "    # Perform `repetitions` times T Bernoulli experiments\n",
    "    decisions = np.random.binomial(1, sigma, size=(repeats, max(Ts)))\n",
    "    change = np.c_[np.zeros(decisions.shape[0]), np.random.uniform(size=(repeats, max(Ts)-1)) <= r]\n",
    "    correlated_decisions = np.ma.array(decisions, mask=change).filled(fill_value=decisions[:,0][:, None])\n",
    "    correlated_decisions[correlated_decisions == 0] = -1\n",
    "\n",
    "    variances[i,:] = np.var(np.cumsum(correlated_decisions, axis=1) >= 0, axis=0)[Ts-1]\n",
    "    \n",
    "df = pd.DataFrame(variances, columns=Ts, index=rho)\n",
    "fig = go.Figure(go.Heatmap(x=df.columns.tolist(), y=df.index.tolist(), z=df.values.tolist(), colorscale=\"amp\"),\n",
    "          layout=go.Layout(title=rf\"$\\text{{Variance of Committee Decision - Correlated Committee Members - Member Decision Variance }}\\sigma^2 = {sigma}$\", \n",
    "                           xaxis=dict(title=r\"$T\\text{ - Committee Size}$\", type=\"category\"),\n",
    "                           yaxis=dict(title=r\"$\\rho\\text{ - Correlation Between Members}$\"),\n",
    "                           width=500, height=300))\n",
    "\n",
    "fig.write_image(\"../figures/correlated_committee_decision.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "### Empirical CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import norm\n",
    "\n",
    "data = np.random.normal(size=10000)\n",
    "frames = []\n",
    "for m in [5,10, 15, 20, 25, 50, 75, 100, 150, 200, 250, 500, 750, 1000,1500, 2000, 2500, 5000, 7500, 10000]:\n",
    "    ecdf = ECDF(data[:m])\n",
    "    frames.append(go.Frame(\n",
    "        data = [\n",
    "            go.Scatter(x=data[:m], y=[-.1]*m, mode=\"markers\", marker=dict(size=5, color=norm.pdf(data[:m])), name=\"Samples\"),\n",
    "            go.Scatter(x=ecdf.x, y=ecdf.y, marker_color=\"black\", name=\"Empirical CDF\"),                \n",
    "            go.Scatter(x=np.linspace(-3,3,100), y=norm.cdf(np.linspace(-3,3,100), 0, 1), mode=\"lines\", \n",
    "                       line=dict(color=\"grey\", dash='dash'), name=\"Theoretical CDF\")],\n",
    "        layout = go.Layout(title=rf\"$\\text{{Empirical CDF of }}m={m}\\text{{ Samples Drawn From }}\\mathcal{{N}}\\left(0,1\\right)$\")\n",
    "    ))\n",
    "\n",
    "fig = go.Figure(data = frames[0].data, frames=frames[1:], \n",
    "                layout=go.Layout(title=frames[0].layout.title,\n",
    "                                 updatemenus=[dict(type=\"buttons\", buttons=[AnimationButtons.play(frame_duration=1000), \n",
    "                                                                            AnimationButtons.pause()])]))\n",
    "\n",
    "\n",
    "animation_to_gif(fig, \"../figures/empirical_cdf.gif\", 700, width=600, height=450)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class StagedAdaBoostClassifier(AdaBoostClassifier):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.sample_weights = []\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        self.sample_weights.append(sample_weight.copy())\n",
    "#         self.res_list.append(super()._boost(iboost, X, y, sample_weight, random_state))\n",
    "#         return self.res_list[-1]\n",
    "        return super()._boost(iboost, X, y, sample_weight, random_state)\n",
    "\n",
    "    def _iteration_callback(self, iboost, X, y, sample_weight, \n",
    "                            estimator_weight = None, estimator_error = None):\n",
    "        self.sample_weights.append(sample_weight.copy())\n",
    "        \n",
    "        \n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Construct dataset of two sets of Gaussian quantiles\n",
    "X1, y1 = make_gaussian_quantiles(cov=2., n_samples=50, n_features=2, n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=50, n_features=2, n_classes=2, random_state=1)\n",
    "X, y = np.concatenate((X1, X2)), np.concatenate((y1, - y2 + 1))\n",
    "\n",
    "\n",
    "# Form grid of points to use for plotting decision boundaries \n",
    "lims = np.array([X.min(axis=0), X.max(axis=0)]).T + np.array([-.2, .2])\n",
    "xx, yy = list(map(np.ravel, np.meshgrid(np.arange(*lims[0], .2), np.arange(*lims[1], .2))))\n",
    "\n",
    "\n",
    "# Fit AdaBoost classifier over training set\n",
    "model = StagedAdaBoostClassifier().fit(X, y)\n",
    "# Retrieve model train error at each iteration of fitting\n",
    "staged_scores = list(model.staged_score(X, y))\n",
    "# Predict labels of grid points at each iteration of fitting\n",
    "staged_predictions = np.array(list(model.staged_predict(np.vstack([xx, yy]).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation frames\n",
    "frames = []\n",
    "for i in range(len(staged_predictions)):\n",
    "    frames.append(go.Frame(\n",
    "        data=[\n",
    "            # Scatter of sample weights\n",
    "            go.Scatter(x=X[:,0], y= X[:,1], mode='markers', showlegend=False, marker=dict(color=y, colorscale=class_colors(2),\n",
    "                       size=np.maximum(230*model.sample_weights[i]+1, np.ones(len(model.sample_weights[i]))*5)),\n",
    "                       xaxis=\"x\", yaxis=\"y\"), \n",
    "            \n",
    "            # Staged decision surface \n",
    "            go.Scatter(x=xx,  y=yy, marker=dict(symbol = \"square\", colorscale=custom, color=staged_predictions[i,:]), \n",
    "                       mode='markers', opacity = 0.4, showlegend=False, xaxis=\"x2\", yaxis=\"y2\"),\n",
    "            \n",
    "            # Scatter of train samples with true class\n",
    "            go.Scatter(x=X[:,0],  y=X[:,1], mode='markers', showlegend=False, xaxis=\"x2\", yaxis=\"y2\",\n",
    "                       marker=dict(color=y, colorscale=class_colors(2), symbol=class_symbols[y])),\n",
    "            \n",
    "            # Scatter of staged score\n",
    "            go.Scatter(x=list(range(i)), y=staged_scores[:i], mode='lines+markers', showlegend=False, marker_color=\"black\",\n",
    "                       xaxis=\"x3\", yaxis=\"y3\")\n",
    "        ],\n",
    "        layout = go.Layout(title = rf\"$\\text{{AdaBoost Training - Iteration }}{i+1}/{len(staged_predictions)}$)\"),\n",
    "        traces=[0, 1, 2, 3]))    \n",
    "\n",
    "    \n",
    "fig = make_subplots(rows=2, cols=2, row_heights=[350, 200],\n",
    "                    subplot_titles=(r\"$\\text{Sample Weights}$\", r\"$\\text{Decisions Boundaries}$\", \n",
    "                                    r\"$\\text{Ensemble Train Accuracy}$\"),\n",
    "                    specs=[[{}, {}], [{\"colspan\": 2}, None]])\\\n",
    "    .add_traces(data=frames[0].data, rows=[1,1,1,2], cols=[1,2,2,1])\\\n",
    "    .update(frames = frames)\\\n",
    "    .update_layout(title=frames[0].layout.title,\n",
    "                   updatemenus = [dict(type=\"buttons\", buttons=[AnimationButtons.play(), AnimationButtons.pause()])], \n",
    "                   width=600, height=550, margin=dict(t=100))\\\n",
    "    .update_yaxes(range=[min(staged_scores)-.1, 1.1], autorange=False, row=2, col=1)\\\n",
    "    .update_xaxes(range=[0, len(frames)], autorange=False, row=2, col=1)\n",
    "\n",
    "animation_to_gif(fig, \"../figures/adaboost.gif\", 1000, width=600, height=550)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}